# -*- coding: utf-8 -*-
"""Hate Speech Detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YO4sSRbT6ZVhGl_2Ila7ZcKNQoLAxJaR

IMPORTING LIBRARIES
"""

import warnings
warnings.filterwarnings('ignore')

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

from google.colab import files
uploaded = files.upload()

df = pd.read_csv('train.csv')
df.head()

df.shape

"""HATE TWEETS"""

hate_tweets = df[df.label == 1]
hate_tweets.head()

"""**NORMAL TWEETS**"""

normal_tweets = df[df.label == 0]
normal_tweets.head()

# Displaying hate words clouds
from os import path
from PIL import Image
from wordcloud import WordCloud,STOPWORDS,ImageColorGenerator
text = " ".join(tweets for tweets in hate_tweets.tweet)
wordcloud = WordCloud(max_font_size = 50,max_words = 100,background_color = 'white').generate(text)
plt.figure(figsize=(20,6))
plt.imshow(wordcloud, interpolation = "bilinear")
plt.axis("off")

sns.countplot(x = 'label',data = df)

"""**Showing the percentage of hate tweets and normal tweets**"""

df_stats = df[['label','tweet']].groupby('label').count().reset_index()
df_stats.columns = ['label','count']
df_stats['percentage'] = (df_stats['count']/df_stats['count'].sum()) * 100
df_stats

"""Data Preprocessing"""

import re
def preprocess(tweet):
  return " ".join(re.sub("(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])", " ",tweet.lower()).split())

df['processed_tweets'] = df['tweet'].apply(preprocess)
df.head()

"""**As the dataset is higly imbalanced, we have to balance it using oversampling.**"""

normal_count = df[df['label'] == 0]['processed_tweets'].count()
hate_count_df = df[df['label'] == 1]
normal_count_df = df[df['label'] == 0]
df_hate_count_oversampled = hate_count_df.sample(normal_count, replace=True)
df_oversampled = pd.concat([normal_count_df, df_hate_count_oversampled], axis=0)

print("Random Over Sampling: ")
print(df_oversampled['label'].value_counts())

"""Splitting data into training and test sets"""

X = df_oversampled['processed_tweets']
y = df_oversampled['label']

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score,classification_report
from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer,TfidfTransformer

"""BUILDING A PIPELINE MODEL TO TEST ALL CLASSIFICATION ALGORITHMS"""

def classify(model,X,y):
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, shuffle=True, stratify=y)

  #Model training
  pipeline_model = Pipeline([('vect',CountVectorizer()),
                             ('tfidf',TfidfTransformer()),
                             ('clf',model)])
  pipeline_model.fit(X_train,y_train)

  pred = pipeline_model.predict(X_test)
  print(classification_report(y_test,pred))
  print('\n')
  print('The accuracy of model is ',accuracy_score(y_test,pred))

"""LOGISTIC REGRESSION"""

from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
classify(model,X,y)

"""SUPPORT VECTOR CLASSIFIER(SVC)"""

from sklearn import svm
model = svm.LinearSVC()
classify(model,X,y)

"""**RANDOM FOREST CLASSIFIER**"""

from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier()
classify(model,X,y)

"""MULTINOMIAL NAIVE BAYES"""

from sklearn.naive_bayes import MultinomialNB
model = MultinomialNB()
classify(model,X,y)

"""So, from all the above algorithms, Random Forest gave the highest accuracy with 99.7% accuracy."""

